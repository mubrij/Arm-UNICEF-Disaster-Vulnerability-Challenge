{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.metrics import f1_score as sklearn_f1\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torchvision\n",
    "from torchvision.transforms import Normalize\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.cuda.amp as amp\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.ops import nms\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "torch.manual_seed(3)\n",
    "torch.cuda.manual_seed(3)\n",
    "torch.cuda.manual_seed_all(3)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(3)\n",
    "random.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/home/unicconaiadmin/Music1/Msa/arm-unicef-disaster-vulnerability-challenge/\"\n",
    "IMG_PATH = \"/home/unicconaiadmin/Music1/Msa/arm-unicef-disaster-vulnerability-challenge/Images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(f\"{PATH}Train.csv\")\n",
    "test = pd.read_csv(f\"{PATH}Test.csv\")\n",
    "ss = pd.read_csv(f\"{PATH}SampleSubmission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>bbox</th>\n",
       "      <th>category_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_w55q2qr62fsk</td>\n",
       "      <td>[122.0, 1.0, 42.0, 30.0]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6356.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_fvldv7o1kn9d</td>\n",
       "      <td>[500.0, 141.0, 74.0, 70.0]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2305.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_5d1r9l1jp7b5</td>\n",
       "      <td>[304.0, 525.0, 54.0, 58.0]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>238.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_45qru79t6s4n</td>\n",
       "      <td>[187.0, 298.0, 44.0, 56.0]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_84cjf2pbqmtv</td>\n",
       "      <td>[349.0, 721.0, 58.0, 59.0]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1362.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          image_id                        bbox  category_id      id\n",
       "0  id_w55q2qr62fsk    [122.0, 1.0, 42.0, 30.0]          2.0  6356.0\n",
       "1  id_fvldv7o1kn9d  [500.0, 141.0, 74.0, 70.0]          2.0  2305.0\n",
       "2  id_5d1r9l1jp7b5  [304.0, 525.0, 54.0, 58.0]          2.0   238.0\n",
       "3  id_45qru79t6s4n  [187.0, 298.0, 44.0, 56.0]          2.0   921.0\n",
       "4  id_84cjf2pbqmtv  [349.0, 721.0, 58.0, 59.0]          2.0  1362.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>bbox</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_w55q2qr62fsk</td>\n",
       "      <td>[122.0, 1.0, 42.0, 30.0]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_fvldv7o1kn9d</td>\n",
       "      <td>[500.0, 141.0, 74.0, 70.0]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_5d1r9l1jp7b5</td>\n",
       "      <td>[304.0, 525.0, 54.0, 58.0]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_45qru79t6s4n</td>\n",
       "      <td>[187.0, 298.0, 44.0, 56.0]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_84cjf2pbqmtv</td>\n",
       "      <td>[349.0, 721.0, 58.0, 59.0]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          image_id                        bbox  category_id\n",
       "0  id_w55q2qr62fsk    [122.0, 1.0, 42.0, 30.0]          2.0\n",
       "1  id_fvldv7o1kn9d  [500.0, 141.0, 74.0, 70.0]          2.0\n",
       "2  id_5d1r9l1jp7b5  [304.0, 525.0, 54.0, 58.0]          2.0\n",
       "3  id_45qru79t6s4n  [187.0, 298.0, 44.0, 56.0]          2.0\n",
       "4  id_84cjf2pbqmtv  [349.0, 721.0, 58.0, 59.0]          2.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=train.drop(columns=['id'])\n",
    "train = train.dropna(subset=['bbox'])\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['x'] = -1\n",
    "train['y'] = -1\n",
    "train['w'] = -1\n",
    "train['h'] = -1\n",
    "\n",
    "def expand_bbox(x):\n",
    "    r = np.array(re.findall(\"([0-9]+[.]?[0-9]*)\", x))\n",
    "    if len(r) == 0:\n",
    "        r = [-1, -1, -1, -1]\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['x', 'y', 'w', 'h']] = np.stack(train['bbox'].apply(lambda x: expand_bbox(x))) ##Lets convert the Box in\n",
    "train['x'] = train['x'].astype(float)                                        #in our desired formate\n",
    "train['y'] = train['y'].astype(float)\n",
    "train['w'] = train['w'].astype(float)\n",
    "train['h'] = train['h'].astype(float)\n",
    "train.drop(columns=['bbox'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['image_id', 'category_id', 'x', 'y', 'w', 'h'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>w</th>\n",
       "      <th>h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_w55q2qr62fsk</td>\n",
       "      <td>2.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_fvldv7o1kn9d</td>\n",
       "      <td>2.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_5d1r9l1jp7b5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>525.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_45qru79t6s4n</td>\n",
       "      <td>2.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>298.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_84cjf2pbqmtv</td>\n",
       "      <td>2.0</td>\n",
       "      <td>349.0</td>\n",
       "      <td>721.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          image_id  category_id      x      y     w     h\n",
       "0  id_w55q2qr62fsk          2.0  122.0    1.0  42.0  30.0\n",
       "1  id_fvldv7o1kn9d          2.0  500.0  141.0  74.0  70.0\n",
       "2  id_5d1r9l1jp7b5          2.0  304.0  525.0  54.0  58.0\n",
       "3  id_45qru79t6s4n          2.0  187.0  298.0  44.0  56.0\n",
       "4  id_84cjf2pbqmtv          2.0  349.0  721.0  58.0  59.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category_id\n",
       "2.0    21330\n",
       "3.0     2348\n",
       "1.0      171\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"category_id\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HouseDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transforms=None):\n",
    "        super().__init__()\n",
    "        self.image_ids = dataframe['image_id'].unique()\n",
    "        self.df = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_id = self.image_ids[index]\n",
    "        records = self.df[self.df['image_id'] == image_id]\n",
    "        try:\n",
    "            image = Image.open(f'{self.image_dir}/{image_id}.tif').convert('RGB')\n",
    "            image = np.array(image, dtype=np.float32) / 255.0\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image_id}: {e}\")\n",
    "            return self.__getitem__(np.random.randint(len(self)))\n",
    "\n",
    "        boxes = records[['x', 'y', 'w', 'h']].values\n",
    "        if (boxes[:, 2] <= 0).any() or (boxes[:, 3] <= 0).any():\n",
    "            return self.__getitem__(np.random.randint(len(self)))\n",
    "        boxes[:, 2] += boxes[:, 0]\n",
    "        boxes[:, 3] += boxes[:, 1]\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        area = torch.as_tensor(area, dtype=torch.float32)\n",
    "        category_id = records['category_id'].values\n",
    "        category_id = torch.as_tensor(category_id, dtype=torch.int64)\n",
    "        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = category_id\n",
    "        target['image_id'] = torch.tensor([index])\n",
    "        target['area'] = area\n",
    "        target['iscrowd'] = iscrowd\n",
    "\n",
    "        if self.transforms:\n",
    "            sample = {\n",
    "                'image': image,\n",
    "                'bboxes': target['boxes'],\n",
    "                'labels': category_id\n",
    "            }\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "            image = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(image)\n",
    "            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "\n",
    "        return image, target, image_id\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_ids.shape[0]\n",
    "\n",
    "    def compute_num_categories(self):\n",
    "        self.num_categories = {}\n",
    "        for image_id in self.image_ids:\n",
    "            records = self.df[self.df['image_id'] == image_id]\n",
    "            num_categories = len(records['category_id'].unique())\n",
    "            self.num_categories[image_id] = num_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.OneOf([A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, val_shift_limit=0.2, p=0.9),\n",
    "                               A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.9),\n",
    "                A.MotionBlur(p=0.2),\n",
    "               # A.CLAHE(p=0.5),\n",
    "                A.RandomFog(p=0.5),\n",
    "                A.Solarize(p=0.5),\n",
    "                A.RandomGamma(p=0.2),\n",
    "                A.RandomScale(scale_limit=0.15, p=0.1),\n",
    "                A.RandomSunFlare(p=0.1),\n",
    "                A.MotionBlur(p=0.2),\n",
    "                A.MedianBlur(blur_limit=3, p=0.1),\n",
    "                A.Blur(blur_limit=3, p=0.1),\n",
    "                                A.Sharpen(p=0.1),],p=1.0),            \n",
    "        A.HorizontalFlip(p=0.7),\n",
    "    #    A.Flip(0.6),\n",
    "        ToTensorV2(p=1.0),\n",
    "    ], bbox_params={'format': 'pascal_voc', 'min_area': 0.1, 'min_visibility': 0, 'label_fields': ['labels']})\n",
    "\n",
    "def get_valid_transform():\n",
    "    return A.Compose([            \n",
    "        A.HorizontalFlip(p=0.6),\n",
    "    #    A.Flip(0.5),\n",
    "        ToTensorV2(p=1.0),\n",
    "    ], bbox_params={'format': 'pascal_voc', 'min_area': 0.1, 'min_visibility': 0, 'label_fields': ['labels']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed = 20\n",
    "    N_folds = 6\n",
    "    train_folds = [0, 1, 2, 3, 4, 5]\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    apex = True  # use half precision\n",
    "    epochs = 50\n",
    "    weights = torch.tensor([0.206119, 0.793881], dtype=torch.float32)\n",
    "    clip_val = 1000.\n",
    "    batch_size =20\n",
    "    gradient_accumulation_steps = 4  \n",
    "    lr = 1e-4\n",
    "    weight_decay = 1e-2\n",
    "    mixed_precision = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=2024):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class style:\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    END = '\\033[0m'\n",
    "    BOLD = '\\033[1m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "sgkf = StratifiedKFold(n_splits=CFG.N_folds, random_state=CFG.seed, shuffle=True)\n",
    "image_ids = train['image_id'].unique()\n",
    "stratify_groups = train.groupby('image_id')['category_id'].first()\n",
    "\n",
    "for fold, (_, test_index) in enumerate(sgkf.split(image_ids, stratify_groups)):\n",
    "    test_image_ids = image_ids[test_index]\n",
    "    train.loc[train['image_id'].isin(test_image_ids), \"fold\"] = fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(cfg, model, train_loader, optimizer, scheduler, epoch):\n",
    "    loss_hist = Averager()\n",
    "    itr = 1\n",
    "    learning_rate_history = []\n",
    "    targets = []\n",
    "    predictions = []\n",
    "    scaler = amp.GradScaler(enabled=cfg.mixed_precision)\n",
    "    model.train()\n",
    "    loss_hist.reset()\n",
    "    model = model.to(cfg.device, non_blocking=True)\n",
    "\n",
    "    for images, targets, image_ids in tqdm(train_loader):\n",
    "        images = [image.to(cfg.device, non_blocking=True) for image in images]\n",
    "        targets = [{k: v.to(cfg.device, non_blocking=True) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        with amp.autocast(enabled=cfg.mixed_precision):\n",
    "            loss_dict = model(images, targets)\n",
    "            \n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        loss_hist.send(loss_value)\n",
    "        \n",
    "        # Gradient Accumulation\n",
    "        losses = losses / cfg.gradient_accumulation_steps\n",
    "        scaler.scale(losses).backward()\n",
    "        if (itr + 1) % cfg.gradient_accumulation_steps == 0:\n",
    "            params = [p for p in model.parameters() if p.requires_grad]\n",
    "            torch.nn.utils.clip_grad_norm_(params, max_norm=cfg.clip_val)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        if itr % 50 == 0:\n",
    "            print(f\"Iteration #{itr} loss: {loss_value}\")\n",
    "        itr += 1\n",
    "        if scheduler is None:\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "        else:\n",
    "            scheduler.step()\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "    print(f\"Epoch #{epoch} loss: {loss_hist.value}\")\n",
    "    learning_rate_history.append(lr)\n",
    "    return learning_rate_history, loss_hist.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for FOLD in CFG.train_folds:\n",
    "\n",
    "    seed_everything(CFG.seed)\n",
    "\n",
    "    # PREPARE DATA\n",
    "    fold_train_data = train[train[\"fold\"] != FOLD].reset_index(drop=True)\n",
    "    fold_valid_data = train[train[\"fold\"] == FOLD].reset_index(drop=True)\n",
    "\n",
    "    display(\n",
    "        pd.merge(\n",
    "            fold_valid_data.groupby(by=[\"category_id\"])[\"image_id\"].count().rename(\"valid\").reset_index(),\n",
    "            fold_train_data.groupby(by=[\"category_id\"])[\"image_id\"].count().rename(\"train\").reset_index(),\n",
    "             on=\"category_id\", how=\"left\").T,)\n",
    "\n",
    "\n",
    "    train_dataset = HouseDataset(fold_train_data, IMG_PATH, get_train_transform())\n",
    "    valid_dataset = HouseDataset(fold_valid_data, IMG_PATH, get_valid_transform())\n",
    "\n",
    "    train_dataset.compute_num_categories()\n",
    "    valid_dataset.compute_num_categories()\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=CFG.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=os.cpu_count(),\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=os.cpu_count(),\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights='FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1')\n",
    "    num_classes = train.category_id.nunique()+1\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.AdamW(params, lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, eta_min=1e-6, T_max =CFG.epochs * len(train_loader),\n",
    "        )\n",
    "\n",
    "    # TRAIN FOLD\n",
    "    learning_rate_history = []\n",
    "\n",
    "    best_score = 0\n",
    "    for epoch in range(0, CFG.epochs):\n",
    "        train_lr = train_epoch(CFG, model, train_loader, optimizer, scheduler, epoch)\n",
    "        learning_rate_history.extend(train_lr)\n",
    "        torch.save(model.state_dict(), f'/home/unicconaiadmin/Music1/Msa/best_model_fold_{FOLD}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Available detection models in torchvision.models.detection:\n",
    "- fasterrcnn_resnet50_fpn\n",
    "- maskrcnn_resnet50_fpn\n",
    "- keypointrcnn_resnet50_fpn\n",
    "- fasterrcnn_mobilenet_v3_large_320_fpn\n",
    "- fasterrcnn_mobilenet_v3_large_fpn\n",
    "- fasterrcnn_resnet50_fpn_v2\n",
    "- maskrcnn_resnet50_fpn_v2\n",
    "- keypointrcnn_resnet50_fpn_v2\n",
    "- retinanet_resnet50_fpn\n",
    "- retinanet_resnet50_fpn_v2\n",
    "- ssd300_vgg16\n",
    "- ssdlite320_mobilenet_v3_large\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
