{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'arm-unicef-disaster-vulnerability-challenge:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4605322%2F7852537%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240426%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240426T045525Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D0888b0002aba628427a66bf5e9ca7d7b381a7e8fa852288282db665ad0031f90f65724a989d0a44f9cf9b47e1e1499a82b9abd03c7af700d6f6ab46cd0b47921889353fc0fc4b6596a17f65424a6d95babe1e9caccbb1d2a3903c531cfb419b6e877881d8786993575ef905cc8686193ed96b72ab30fe40f1dfd7e619e90f780f849c7f97980109679b23764097390ab1b9d6054b2d4a1aa46bfba9ce24bb24f37b42bab10438d224864f5c67733934574da7148e6d0703bcad8665c8406bd5425f2ba9faf9b0e4aef268d540cb21264d631914c07dc6b0e79974db9a3b888429abbb3369293f09c6d2dbbe9d378045a0752da06a13db1467caf513c080f2cec'\n",
        "\n",
        "KAGGLE_INPUT_PATH = 'input'\n",
        "KAGGLE_WORKING_PATH = 'working'\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2024-04-01T20:42:55.615642Z",
          "iopub.status.busy": "2024-04-01T20:42:55.615261Z",
          "iopub.status.idle": "2024-04-01T20:42:55.624634Z",
          "shell.execute_reply": "2024-04-01T20:42:55.623643Z",
          "shell.execute_reply.started": "2024-04-01T20:42:55.615616Z"
        },
        "id": "fp6ybIVFwi9V",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'ValidationInfo' from 'pydantic' (/home/unicconaiadmin/.local/lib/python3.10/site-packages/pydantic/__init__.cpython-310-x86_64-linux-gnu.so)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01malbumentations\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mA\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01malbumentations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ToTensorV2\n\u001b[1;32m     32\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m3\u001b[39m)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/albumentations/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.4.4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maugmentations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserialization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/albumentations/augmentations/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblur\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblur\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcrops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/albumentations/augmentations/blur/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/albumentations/augmentations/blur/functional.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01malbumentations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maugmentations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convolve\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01malbumentations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maugmentations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m scale\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01malbumentations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maugmentations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     11\u001b[0m     _maybe_process_in_chunks,\n\u001b[1;32m     12\u001b[0m     clipped,\n\u001b[1;32m     13\u001b[0m     preserve_shape,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblur\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedian_blur\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgaussian_blur\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglass_blur\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefocus\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcentral_zoom\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzoom_blur\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/albumentations/augmentations/geometric/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrotate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/albumentations/augmentations/geometric/resize.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Field, ValidationInfo, field_validator\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01malbumentations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InterpolationType, ProbabilityType\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01malbumentations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms_interface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseTransformInitSchema, DualTransform\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'ValidationInfo' from 'pydantic' (/home/unicconaiadmin/.local/lib/python3.10/site-packages/pydantic/__init__.cpython-310-x86_64-linux-gnu.so)"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "import re\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import torchvision\n",
        "from torchvision.transforms import Normalize\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.ops import nms\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data.sampler import SequentialSampler\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "\n",
        "torch.manual_seed(3)\n",
        "torch.cuda.manual_seed(3)\n",
        "torch.cuda.manual_seed_all(3)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(3)\n",
        "random.seed(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-01T20:42:58.540268Z",
          "iopub.status.busy": "2024-04-01T20:42:58.539896Z",
          "iopub.status.idle": "2024-04-01T20:42:58.544654Z",
          "shell.execute_reply": "2024-04-01T20:42:58.543674Z",
          "shell.execute_reply.started": "2024-04-01T20:42:58.54024Z"
        },
        "id": "d1P0I9WUwi9X",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "PATH = \"/home/unicconaiadmin/Music1/Msa/\"\n",
        "IMG_PATH = \"Images/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-01T20:43:00.738625Z",
          "iopub.status.busy": "2024-04-01T20:43:00.738262Z",
          "iopub.status.idle": "2024-04-01T20:43:00.786953Z",
          "shell.execute_reply": "2024-04-01T20:43:00.785961Z",
          "shell.execute_reply.started": "2024-04-01T20:43:00.738595Z"
        },
        "id": "rJ3D01P8wi9Y",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv(f\"{PATH}Train.csv\")\n",
        "test = pd.read_csv(f\"{PATH}Test.csv\")\n",
        "#ss = pd.read_csv(f\"{PATH}SampleSubmission.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ss = pd.read_csv(\"/home/unicconaiadmin/Music1/Msa/arm-unicef-disaster-vulnerability-challenge/SampleSubmission.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-01T20:43:04.195092Z",
          "iopub.status.busy": "2024-04-01T20:43:04.194238Z",
          "iopub.status.idle": "2024-04-01T20:43:04.206923Z",
          "shell.execute_reply": "2024-04-01T20:43:04.206157Z",
          "shell.execute_reply.started": "2024-04-01T20:43:04.19506Z"
        },
        "id": "RKEh1MGBwi9Y",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train=train.drop(columns=['id'])\n",
        "train = train.dropna(subset=['bbox'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-01T20:43:09.521857Z",
          "iopub.status.busy": "2024-04-01T20:43:09.521232Z",
          "iopub.status.idle": "2024-04-01T20:43:09.529264Z",
          "shell.execute_reply": "2024-04-01T20:43:09.528362Z",
          "shell.execute_reply.started": "2024-04-01T20:43:09.521828Z"
        },
        "id": "7ABbPunkwi9p",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train['x'] = -1\n",
        "train['y'] = -1\n",
        "train['w'] = -1\n",
        "train['h'] = -1\n",
        "\n",
        "def expand_bbox(x):\n",
        "    r = np.array(re.findall(\"([0-9]+[.]?[0-9]*)\", x))\n",
        "    if len(r) == 0:\n",
        "        r = [-1, -1, -1, -1]\n",
        "    return r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-01T20:43:13.956647Z",
          "iopub.status.busy": "2024-04-01T20:43:13.956285Z",
          "iopub.status.idle": "2024-04-01T20:43:14.179777Z",
          "shell.execute_reply": "2024-04-01T20:43:14.178963Z",
          "shell.execute_reply.started": "2024-04-01T20:43:13.956622Z"
        },
        "id": "LQsyBrUYwi9q",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train[['x', 'y', 'w', 'h']] = np.stack(train['bbox'].apply(lambda x: expand_bbox(x))) ##Lets convert the Box in\n",
        "train['x'] = train['x'].astype(float)                                        #in our desired formate\n",
        "train['y'] = train['y'].astype(float)\n",
        "train['w'] = train['w'].astype(float)\n",
        "train['h'] = train['h'].astype(float)\n",
        "train.drop(columns=['bbox'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-01T20:28:21.204441Z",
          "iopub.status.busy": "2024-04-01T20:28:21.203707Z",
          "iopub.status.idle": "2024-04-01T20:28:21.211505Z",
          "shell.execute_reply": "2024-04-01T20:28:21.210622Z",
          "shell.execute_reply.started": "2024-04-01T20:28:21.204409Z"
        },
        "id": "F0Or27Niwi9r",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-01T20:43:26.4115Z",
          "iopub.status.busy": "2024-04-01T20:43:26.41075Z",
          "iopub.status.idle": "2024-04-01T20:43:26.518974Z",
          "shell.execute_reply": "2024-04-01T20:43:26.518166Z",
          "shell.execute_reply.started": "2024-04-01T20:43:26.411471Z"
        },
        "id": "W_LKBqIOwi9s",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "image_ids = train['image_id'].unique()\n",
        "# Stratified split based on the 'category_id' column\n",
        "train_ids, valid_ids = train_test_split(image_ids, test_size=0.1, stratify=train.groupby('image_id')['category_id'].apply(lambda x: x.iloc[0]), random_state=3)\n",
        "\n",
        "# Filter dataframe based on split\n",
        "valid_df = train[train['image_id'].isin(valid_ids)]\n",
        "train_df = train[train['image_id'].isin(train_ids)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-01T20:43:32.029084Z",
          "iopub.status.busy": "2024-04-01T20:43:32.028727Z",
          "iopub.status.idle": "2024-04-01T20:43:32.042567Z",
          "shell.execute_reply": "2024-04-01T20:43:32.041617Z",
          "shell.execute_reply.started": "2024-04-01T20:43:32.029054Z"
        },
        "id": "YYfn3Axbwi9s",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(valid_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-01T20:43:35.600602Z",
          "iopub.status.busy": "2024-04-01T20:43:35.599888Z",
          "iopub.status.idle": "2024-04-01T20:43:35.611202Z",
          "shell.execute_reply": "2024-04-01T20:43:35.61034Z",
          "shell.execute_reply.started": "2024-04-01T20:43:35.600572Z"
        },
        "id": "V1BlgYijwi9t",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-01T20:43:39.266908Z",
          "iopub.status.busy": "2024-04-01T20:43:39.266542Z",
          "iopub.status.idle": "2024-04-01T20:43:39.283023Z",
          "shell.execute_reply": "2024-04-01T20:43:39.282315Z",
          "shell.execute_reply.started": "2024-04-01T20:43:39.266878Z"
        },
        "id": "JUNHxf3Kwi9t",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class HouseDataset(Dataset):\n",
        "    def __init__(self, dataframe, image_dir, transforms=None):\n",
        "        super().__init__()\n",
        "        self.image_ids = dataframe['image_id'].unique()\n",
        "        self.df = dataframe\n",
        "        self.image_dir = image_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        image_id = self.image_ids[index]\n",
        "        records = self.df[self.df['image_id'] == image_id]\n",
        "        try:\n",
        "            image = Image.open(f'{self.image_dir}/{image_id}.tif').convert('RGB')\n",
        "            image = np.array(image, dtype=np.float32) / 255.0\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {image_id}: {e}\")\n",
        "            return self.__getitem__(np.random.randint(len(self)))\n",
        "\n",
        "        boxes = records[['x', 'y', 'w', 'h']].values\n",
        "        if (boxes[:, 2] <= 0).any() or (boxes[:, 3] <= 0).any():\n",
        "            return self.__getitem__(np.random.randint(len(self)))\n",
        "        boxes[:, 2] += boxes[:, 0]\n",
        "        boxes[:, 3] += boxes[:, 1]\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        area = torch.as_tensor(area, dtype=torch.float32)\n",
        "        category_id = records['category_id'].values\n",
        "        category_id = torch.as_tensor(category_id, dtype=torch.int64)\n",
        "        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n",
        "        target = {}\n",
        "        target['boxes'] = boxes\n",
        "        target['labels'] = category_id\n",
        "        target['image_id'] = torch.tensor([index])\n",
        "        target['area'] = area\n",
        "        target['iscrowd'] = iscrowd\n",
        "\n",
        "        if self.transforms:\n",
        "            sample = {\n",
        "                'image': image,\n",
        "                'bboxes': target['boxes'],\n",
        "                'labels': category_id\n",
        "            }\n",
        "            sample = self.transforms(**sample)\n",
        "            image = sample['image']\n",
        "            image = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(image)\n",
        "            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
        "\n",
        "        return image, target, image_id\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.image_ids.shape[0]\n",
        "\n",
        "    def compute_num_categories(self):\n",
        "        self.num_categories = {}\n",
        "        for image_id in self.image_ids:\n",
        "            records = self.df[self.df['image_id'] == image_id]\n",
        "            num_categories = len(records['category_id'].unique())\n",
        "            self.num_categories[image_id] = num_categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-01T20:43:43.100162Z",
          "iopub.status.busy": "2024-04-01T20:43:43.099782Z",
          "iopub.status.idle": "2024-04-01T20:43:43.10984Z",
          "shell.execute_reply": "2024-04-01T20:43:43.108904Z",
          "shell.execute_reply.started": "2024-04-01T20:43:43.100133Z"
        },
        "id": "YhMvGjsHwi9u",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def get_train_transform():\n",
        "    return A.Compose([\n",
        "        A.OneOf([A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, val_shift_limit=0.2, p=0.9),\n",
        "                               A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.9),\n",
        "                A.MotionBlur(p=0.2),\n",
        "                A.MedianBlur(blur_limit=3, p=0.1),\n",
        "                A.Blur(blur_limit=3, p=0.1),\n",
        "                                A.Sharpen(p=0.1),],p=1.0),            \n",
        "        A.HorizontalFlip(p=0.7),\n",
        "    #    A.Flip(0.6),\n",
        "        ToTensorV2(p=1.0),\n",
        "    ], bbox_params={'format': 'pascal_voc', 'min_area': 0, 'min_visibility': 0, 'label_fields': ['labels']})\n",
        "\n",
        "def get_valid_transform():\n",
        "    return A.Compose([            \n",
        "        A.HorizontalFlip(p=0.6),\n",
        "    #    A.Flip(0.5),\n",
        "        ToTensorV2(p=1.0),\n",
        "    ], bbox_params={'format': 'pascal_voc', 'min_area': 0, 'min_visibility': 0, 'label_fields': ['labels']})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-01T20:43:46.91237Z",
          "iopub.status.busy": "2024-04-01T20:43:46.911985Z",
          "iopub.status.idle": "2024-04-01T20:43:46.918663Z",
          "shell.execute_reply": "2024-04-01T20:43:46.917787Z",
          "shell.execute_reply.started": "2024-04-01T20:43:46.912339Z"
        },
        "id": "YgPPYo_-wi9v",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class Averager:\n",
        "    def __init__(self):\n",
        "        self.current_total = 0.0\n",
        "        self.iterations = 0.0\n",
        "\n",
        "    def send(self, value):\n",
        "        self.current_total += value\n",
        "        self.iterations += 1\n",
        "\n",
        "    @property\n",
        "    def value(self):\n",
        "        if self.iterations == 0:\n",
        "            return 0\n",
        "        else:\n",
        "            return 1.0 * self.current_total / self.iterations\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_total = 0.0\n",
        "        self.iterations = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-01T20:43:51.175109Z",
          "iopub.status.busy": "2024-04-01T20:43:51.174413Z",
          "iopub.status.idle": "2024-04-01T20:44:00.746153Z",
          "shell.execute_reply": "2024-04-01T20:44:00.745098Z",
          "shell.execute_reply.started": "2024-04-01T20:43:51.175078Z"
        },
        "id": "kbkQRV1zwi9v",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "train_dataset = HouseDataset(train_df, IMG_PATH, get_train_transform())\n",
        "valid_dataset = HouseDataset(valid_df, IMG_PATH, get_valid_transform())\n",
        "\n",
        "train_dataset.compute_num_categories()\n",
        "valid_dataset.compute_num_categories()\n",
        "\n",
        "indices = torch.randperm(len(train_dataset)).tolist()\n",
        "\n",
        "train_data_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=9,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "valid_data_loader = DataLoader(\n",
        "    valid_dataset,\n",
        "    batch_size=9,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-01T20:44:14.368225Z",
          "iopub.status.busy": "2024-04-01T20:44:14.367494Z",
          "iopub.status.idle": "2024-04-01T20:44:15.228416Z",
          "shell.execute_reply": "2024-04-01T20:44:15.227515Z",
          "shell.execute_reply.started": "2024-04-01T20:44:14.36819Z"
        },
        "id": "O21AS3ZTwi90",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights='FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-01T20:44:19.267277Z",
          "iopub.status.busy": "2024-04-01T20:44:19.266415Z",
          "iopub.status.idle": "2024-04-01T20:44:19.272886Z",
          "shell.execute_reply": "2024-04-01T20:44:19.272141Z",
          "shell.execute_reply.started": "2024-04-01T20:44:19.267246Z"
        },
        "id": "h_4CJ86Pwi91",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "num_classes = train.category_id.nunique()+1\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-01T20:44:22.820061Z",
          "iopub.status.busy": "2024-04-01T20:44:22.819219Z",
          "iopub.status.idle": "2024-04-01T20:44:22.824287Z",
          "shell.execute_reply": "2024-04-01T20:44:22.823422Z",
          "shell.execute_reply.started": "2024-04-01T20:44:22.820031Z"
        },
        "id": "RqW95WDawi91",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "cpu_device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-01T20:44:25.891286Z",
          "iopub.status.busy": "2024-04-01T20:44:25.890348Z",
          "iopub.status.idle": "2024-04-01T20:44:25.898072Z",
          "shell.execute_reply": "2024-04-01T20:44:25.896992Z",
          "shell.execute_reply.started": "2024-04-01T20:44:25.891256Z"
        },
        "id": "-CEwmKGDwi91",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def sample_batch(data_loader, min_categories=2, max_categories=3, batch_size=16):\n",
        "    while True:\n",
        "        images, targets, image_ids = next(iter(data_loader))\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        category_labels = [data_loader.dataset.num_categories[image_id] for image_id in image_ids]\n",
        "        num_categories = len(set(category_labels))\n",
        "        if min_categories <= num_categories <= max_categories:\n",
        "            return images[:batch_size], targets[:batch_size], image_ids[:batch_size], category_labels[:batch_size]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-01T20:44:31.522631Z",
          "iopub.status.busy": "2024-04-01T20:44:31.522271Z",
          "iopub.status.idle": "2024-04-01T20:44:32.376242Z",
          "shell.execute_reply": "2024-04-01T20:44:32.375414Z",
          "shell.execute_reply.started": "2024-04-01T20:44:31.522607Z"
        },
        "id": "dNbVT4ifwi92",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "images, targets, image_ids, category_labels = sample_batch(train_data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-01T20:44:34.754909Z",
          "iopub.status.busy": "2024-04-01T20:44:34.754226Z",
          "iopub.status.idle": "2024-04-01T20:44:35.146944Z",
          "shell.execute_reply": "2024-04-01T20:44:35.145993Z",
          "shell.execute_reply.started": "2024-04-01T20:44:34.754876Z"
        },
        "id": "gvQq-9Dcwi92",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "colors = {1: (0, 255, 0), 2: (0, 0, 255), 3: (255, 0, 0)}  # define colors for each category\n",
        "boxes = targets[2]['boxes'].cpu().numpy().astype(np.int32)\n",
        "sample = images[2].permute(1,2,0).cpu().numpy()\n",
        "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
        "\n",
        "for i in range(len(boxes)):\n",
        "    box = boxes[i]\n",
        "    label = category_labels[i]\n",
        "    color = colors[label]\n",
        "    cv2.rectangle(sample,\n",
        "                  (box[0], box[1]),\n",
        "                  (box[2], box[3]),\n",
        "                  color, 3)\n",
        "\n",
        "ax.set_axis_off()\n",
        "ax.imshow(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-01T20:44:52.10214Z",
          "iopub.status.busy": "2024-04-01T20:44:52.101484Z",
          "iopub.status.idle": "2024-04-01T20:44:52.169274Z",
          "shell.execute_reply": "2024-04-01T20:44:52.168514Z",
          "shell.execute_reply.started": "2024-04-01T20:44:52.102091Z"
        },
        "id": "ulBtnltDwi93",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model.to(device)\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.Adagrad(params, lr=0.001, lr_decay=0.00002, weight_decay=0.0005)\n",
        "#lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, verbose=True, step_size=30, gamma=0.15)\n",
        "lr_scheduler = None\n",
        "num_epochs = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-01T20:44:56.663831Z",
          "iopub.status.busy": "2024-04-01T20:44:56.663088Z"
        },
        "id": "9y2mWiZlwi93",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "loss_hist = Averager()\n",
        "itr = 1\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    loss_hist.reset()\n",
        "\n",
        "    for images, targets, image_ids in tqdm(train_data_loader):\n",
        "\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        loss_value = losses.item()\n",
        "\n",
        "        loss_hist.send(loss_value)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if itr % 50 == 0:\n",
        "            print(f\"Iteration #{itr} loss: {loss_value}\")\n",
        "\n",
        "        itr += 1\n",
        "\n",
        "    # update the learning rate\n",
        "    if lr_scheduler is not None:\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    print(f\"Epoch #{epoch} loss: {loss_hist.value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKEYDTFnwi93",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# get a single batch of images, targets, and image ids from the validation data loader\n",
        "with torch.no_grad():\n",
        "    images, targets, image_ids = next(iter(valid_data_loader))\n",
        "\n",
        "# move the images and targets to the device\n",
        "images = list(img.to(device) for img in images)\n",
        "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "# forward pass through the model\n",
        "outputs = model(images)\n",
        "\n",
        "# move the outputs to the CPU\n",
        "outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
        "\n",
        "# get the predicted boxes and labels for the first image\n",
        "boxes = outputs[1]['boxes'].detach().numpy()\n",
        "labels = outputs[1]['labels'].numpy()\n",
        "scores = outputs[1]['scores'].detach().numpy()\n",
        "\n",
        "# filter out boxes with a score <= 0.5\n",
        "filtered_idx = scores > 0.5\n",
        "boxes = boxes[filtered_idx]\n",
        "labels = labels[filtered_idx]\n",
        "\n",
        "# get the ground truth boxes and labels for the first image\n",
        "gt_boxes = targets[1]['boxes'].cpu().detach().numpy()\n",
        "gt_labels = targets[1]['labels'].cpu().numpy()\n",
        "\n",
        "# get the first image and move it to the CPU\n",
        "image = images[1].to(cpu_device).numpy()\n",
        "image = np.transpose(image, (1, 2, 0))\n",
        "\n",
        "# plot the image\n",
        "plt.imshow(image)\n",
        "\n",
        "# plot the ground truth boxes in green\n",
        "for box, label in zip(gt_boxes, gt_labels):\n",
        "    box = box.astype(np.int32)\n",
        "    plt.gca().add_patch(plt.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], fill=False, edgecolor='green', linewidth=2))\n",
        "\n",
        "# plot the predicted boxes in red\n",
        "for box, label in zip(boxes, labels):\n",
        "    box = box.astype(np.int32)\n",
        "    plt.gca().add_patch(plt.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], fill=False, edgecolor='red', linewidth=2))\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set the threshold for probability\n",
        "probability_threshold = 0.45\n",
        "\n",
        "image_id_list = []  # changed variable name\n",
        "category_ids = []\n",
        "bboxes = []\n",
        "# loop over the test data loader\n",
        "for images, targets, image_ids in tqdm(valid_data_loader):\n",
        "\n",
        "    # move the images to the device\n",
        "    images = list(img.to(device) for img in images)\n",
        "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "    # forward pass through the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images)\n",
        "\n",
        "    # move the outputs to the CPU\n",
        "    outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
        "\n",
        "    # loop over the outputs for each image\n",
        "    for i, output in enumerate(outputs):\n",
        "\n",
        "        # filter out predictions with confidence scores less than the threshold\n",
        "        boxes = output['boxes'][output['scores'] >= probability_threshold].cpu().numpy()\n",
        "        labels = output['labels'][output['scores'] >= probability_threshold].cpu().numpy()\n",
        "\n",
        "        # get the image id\n",
        "        image_id = image_ids[i]\n",
        "\n",
        "        # loop over the filtered boxes and labels\n",
        "        for box, label in zip(boxes, labels):\n",
        "\n",
        "            # append the image id, category id, and bounding box to the lists\n",
        "            image_id_list.append(image_id)  # changed variable name\n",
        "            category_ids.append(label)\n",
        "            bboxes.append(box.tolist())\n",
        "\n",
        "\n",
        "# create a dataframe from the validation output\n",
        "validation_output = pd.DataFrame({\n",
        "    'image_id': image_id_list,  # changed variable name\n",
        "    'category_id': category_ids,\n",
        "    'bbox': bboxes\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-xukJw7wi94",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "validsub = pd.DataFrame(columns=['image_id', 'Target'])\n",
        "validtest = pd.DataFrame(columns=['image_id', 'Target'])\n",
        "# Iterate through each unique image_id in the original DataFrame\n",
        "for image_id in valid_df['image_id'].unique():\n",
        "    # Iterate through each category_id (assuming you have 4 categories)\n",
        "    for category_id in range(1, 4):\n",
        "        # Create a new row for each image_id and category_id combination\n",
        "        new_row = {'image_id': f\"{image_id}_{category_id}\", 'Target': valid_df[(valid_df['image_id'] == image_id) & (valid_df['category_id'] == category_id)].shape[0]}\n",
        "        validsub = pd.concat([validsub, pd.DataFrame([new_row])], ignore_index=True)\n",
        "\n",
        "for image_id in validation_output['image_id'].unique():\n",
        "    # Iterate through each category_id (assuming you have 4 categories)\n",
        "    for category_id in range(1, 4):\n",
        "        # Create a new row for each image_id and category_id combination\n",
        "        new_row = {'image_id': f\"{image_id}_{category_id}\", 'Target': validation_output[(validation_output['image_id'] == image_id) & (validation_output['category_id'] == category_id)].shape[0]}\n",
        "        validtest = pd.concat([validtest, pd.DataFrame([new_row])], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifHIdvqZwi95",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "validsub.head(7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZF9pSBMMwi95",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "validtest.head(7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTZrdLJLwi95",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#mae = mean_absolute_error(validsub['Target'], validtest['Target'])\n",
        "#print(\"Mean Absolute Error:\", mae)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'fasterrcnn_resnet50_fpn.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.path.exists(\"/home/unicconaiadmin/Music1/Msa/best_model_fold_1.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights='FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1')\n",
        "num_classes = train.category_id.nunique()+1\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "model.load_state_dict(torch.load('/home/unicconaiadmin/Music1/Msa/fasternew_docker_model_1.pth'))\n",
        "model.to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMI-ZNFrwi96",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class HouseDatasetTest(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, image_dir, transforms=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.image_ids = dataframe['image_id'].unique()\n",
        "        self.df = dataframe\n",
        "        self.image_dir = image_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "\n",
        "        image_id = self.image_ids[index]\n",
        "        records = self.df[self.df['image_id'] == image_id]\n",
        "\n",
        "        image = cv2.imread(f'{self.image_dir}/{image_id}.tif', cv2.IMREAD_COLOR)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        image /= 255.0\n",
        "\n",
        "        if self.transforms:\n",
        "            sample = {'image': image}\n",
        "            sample = self.transforms(**sample)\n",
        "            image = sample['image']\n",
        "            image = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(image)\n",
        "        return image, image_id\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.image_ids.shape[0]\n",
        "\n",
        "def get_test_transform():\n",
        "    return A.Compose([\n",
        "        ToTensorV2(p=1.0)\n",
        "    ])\n",
        "\n",
        "# create the test dataset and DataLoader\n",
        "test_dataset = HouseDatasetTest(test, IMG_PATH, get_test_transform())\n",
        "\n",
        "test_data_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRFdXWuWwi96",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Set the threshold for probability\n",
        "probability_threshold = 0.5\n",
        "counter = 0\n",
        "# create empty lists to store the submission output\n",
        "submission_image_ids = []\n",
        "submission_category_ids = []\n",
        "submission_bboxes = []\n",
        "\n",
        "# loop over the test data loader\n",
        "for images, image_ids in tqdm(test_data_loader):\n",
        "\n",
        "    # move the images to the device\n",
        "    images = list(img.to(device) for img in images)\n",
        "\n",
        "    # forward pass through the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images)\n",
        "\n",
        "    # move the outputs to the CPU\n",
        "    outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
        "\n",
        "    # loop over the outputs for each image\n",
        "    for i, output in enumerate(outputs):\n",
        "\n",
        "        # filter out predictions with confidence scores less than the threshold\n",
        "        filtered_boxes = output['boxes'][output['scores'] >= probability_threshold].cpu().numpy()\n",
        "        filtered_labels = output['labels'][output['scores'] >= probability_threshold].cpu().numpy()\n",
        "\n",
        "        # get the image id\n",
        "        image_id = image_ids[i]\n",
        "\n",
        "        # loop over the filtered boxes and labels\n",
        "        for box, label in zip(filtered_boxes, filtered_labels):\n",
        "\n",
        "            # append the image id, category id, and bounding box to the submission lists\n",
        "            submission_image_ids.append(image_id)\n",
        "            submission_category_ids.append(label)\n",
        "            submission_bboxes.append(box.tolist())\n",
        "\n",
        "        # visualize the first 5 images\n",
        "        if counter < 5:\n",
        "            # get the first image and move it to the CPU\n",
        "            image = images[i].to(cpu_device).numpy()\n",
        "            image = np.transpose(image, (1, 2, 0))\n",
        "\n",
        "            # plot the image\n",
        "            plt.imshow(image)\n",
        "\n",
        "            # plot the predicted boxes in red\n",
        "            for box in filtered_boxes:\n",
        "                box = box.astype(np.int32)\n",
        "                plt.gca().add_patch(plt.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], fill=False, edgecolor='red', linewidth=2))\n",
        "\n",
        "            # show the plot\n",
        "            plt.show()\n",
        "\n",
        "            # increment the counter\n",
        "            counter += 1\n",
        "\n",
        "# create a dataframe from the submission output\n",
        "submission_df = pd.DataFrame({\n",
        "    'image_id': submission_image_ids,\n",
        "    'category_id': submission_category_ids,\n",
        "    'bbox': submission_bboxes\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4LQsbnkwi97",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "submission_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmmNr0sLwi97",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "sub = pd.DataFrame(columns=['image_id', 'Target'])\n",
        "# Iterate through each unique image_id in the original DataFrame\n",
        "for image_id in submission_df['image_id'].unique():\n",
        "    # Iterate through each category_id (assuming you have 4 categories)\n",
        "    for category_id in range(1, 4):\n",
        "        # Create a new row for each image_id and category_id combination\n",
        "        new_row = {'image_id': f\"{image_id}_{category_id}\", 'Target': submission_df[(submission_df['image_id'] == image_id) & (submission_df['category_id'] == category_id)].shape[0]}\n",
        "        sub = pd.concat([sub, pd.DataFrame([new_row])], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwiTteCKwi97",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "sub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SMVkt6Zwi98",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "merged_df = sub.set_index('image_id').combine_first(ss.set_index('image_id')).reset_index()\n",
        "merged_df = merged_df.set_index('image_id').reindex(ss['image_id']).reset_index()\n",
        "\n",
        "# Display the merged dataframe\n",
        "merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMaJ2LOvwi98",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "(merged_df['Target'] != 0).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXuMI35Ewi98",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "merged_df.to_csv('11.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAwMrGXwwi99",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'fasterrcnn_resnet50_fpn.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Arm UNICEF Disaster Vulnerability Challenge",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 4605322,
          "sourceId": 7852537,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30664,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
