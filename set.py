import numpy as np
import pandas as pd
import random
from tqdm import tqdm
from IPython.display import display
import os
import re

import torch
from torch.utils.data import Dataset

from sklearn.metrics import f1_score as sklearn_f1
from sklearn.model_selection import StratifiedKFold
import torchvision
from torchvision.transforms import Normalize
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import torch.nn as nn
import torch.cuda.amp as amp
from torchvision.models.detection import FasterRCNN
from torchvision.models.detection.rpn import AnchorGenerator
from torchvision.ops import nms

from torch.utils.data import DataLoader, Dataset
from torch.utils.data.sampler import SequentialSampler
from matplotlib import pyplot as plt
from PIL import Image
import albumentations as A
from albumentations.pytorch.transforms import ToTensorV2

torch.manual_seed(3)
torch.cuda.manual_seed(3)
torch.cuda.manual_seed_all(3)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
np.random.seed(3)
random.seed(3)

IMG_PATH = "Images"

train = pd.read_csv("Train.csv")
test = pd.read_csv("Test.csv")

train = train.drop(columns=['id'])
train = train.dropna(subset=['bbox'])

train['x'] = -1
train['y'] = -1
train['w'] = -1
train['h'] = -1

def expand_bbox(x):
    r = np.array(re.findall("([0-9]+[.]?[0-9]*)", x))
    if len(r) == 0:
        r = [-1, -1, -1, -1]
    return r


train[['x', 'y', 'w', 'h']] = np.stack(train['bbox'].apply(lambda x: expand_bbox(x))) ##Lets convert the Box in
train['x'] = train['x'].astype(float)                                        #in our desired formate
train['y'] = train['y'].astype(float)
train['w'] = train['w'].astype(float)
train['h'] = train['h'].astype(float)
train.drop(columns=['bbox'], inplace=True)

class HouseDataset(Dataset):
    def __init__(self, dataframe, image_dir, transforms=None):
        super().__init__()
        self.image_ids = dataframe['image_id'].unique()
        self.df = dataframe
        self.image_dir = image_dir
        self.transforms = transforms

    def __getitem__(self, index: int):
        image_id = self.image_ids[index]
        records = self.df[self.df['image_id'] == image_id]
        try:
            image = Image.open(f'{self.image_dir}/{image_id}.tif').convert('RGB')
            image = np.array(image, dtype=np.float32) / 255.0
        except Exception as e:
            print(f"Error processing image {image_id}: {e}")
            return self.__getitem__(np.random.randint(len(self)))

        boxes = records[['x', 'y', 'w', 'h']].values
        if (boxes[:, 2] <= 0).any() or (boxes[:, 3] <= 0).any():
            return self.__getitem__(np.random.randint(len(self)))
        boxes[:, 2] += boxes[:, 0]
        boxes[:, 3] += boxes[:, 1]
        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])
        area = torch.as_tensor(area, dtype=torch.float32)
        category_id = records['category_id'].values
        category_id = torch.as_tensor(category_id, dtype=torch.int64)
        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)
        target = {}
        target['boxes'] = boxes
        target['labels'] = category_id
        target['image_id'] = torch.tensor([index])
        target['area'] = area
        target['iscrowd'] = iscrowd

        if self.transforms:
            sample = {
                'image': image,
                'bboxes': target['boxes'],
                'labels': category_id
            }
            sample = self.transforms(**sample)
            image = sample['image']
            image = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(image)
            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)

        return image, target, image_id

    def __len__(self) -> int:
        return self.image_ids.shape[0]

    def compute_num_categories(self):
        self.num_categories = {}
        for image_id in self.image_ids:
            records = self.df[self.df['image_id'] == image_id]
            num_categories = len(records['category_id'].unique())
            self.num_categories[image_id] = num_categories



class Averager:
    def __init__(self):
        self.current_total = 0.0
        self.iterations = 0.0

    def send(self, value):
        self.current_total += value
        self.iterations += 1

    @property
    def value(self):
        if self.iterations == 0:
            return 0
        else:
            return 1.0 * self.current_total / self.iterations

    def reset(self):
        self.current_total = 0.0
        self.iterations = 0.0
        
class CFG:
    seed = 20
    N_folds = 6
    train_folds = [0, 1, 2, 3, 4, 5]
    device = "cuda" if torch.cuda.is_available() else "cpu"
    apex = True  # use half precision
    epochs = 50
    weights = torch.tensor([0.206119, 0.793881], dtype=torch.float32)
    clip_val = 1000.
    batch_size =30
    gradient_accumulation_steps = 4  
    lr = 1e-4
    weight_decay = 1e-2
    mixed_precision = True

def get_train_transform():
    return A.Compose([
        A.OneOf([A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, val_shift_limit=0.2, p=0.9),
                               A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.9),
              #  A.MotionBlur(p=0.2),
               # A.CLAHE(p=0.5),
               # A.RandomFog(p=0.5),
                #A.Solarize(p=0.5),
                #A.RandomGamma(p=0.2),
                A.RandomScale(scale_limit=0.15, p=0.1),
                A.RandomSunFlare(p=0.1),
                A.MotionBlur(p=0.2),
                A.MedianBlur(blur_limit=3, p=0.1),
                A.Blur(blur_limit=3, p=0.1),
                                A.Sharpen(p=0.1),],p=1.0),            
        A.HorizontalFlip(p=0.7),
    #    A.Flip(0.6),
        ToTensorV2(p=1.0),
    ], bbox_params={'format': 'pascal_voc', 'min_area': 0.1, 'min_visibility': 0, 'label_fields': ['labels']})

def get_valid_transform():
    return A.Compose([            
        A.HorizontalFlip(p=0.6),
    #    A.Flip(0.5),
        ToTensorV2(p=1.0),
    ], bbox_params={'format': 'pascal_voc', 'min_area': 0.1, 'min_visibility': 0, 'label_fields': ['labels']})


def collate_fn(batch):
    return tuple(zip(*batch))  

def seed_everything(seed=2024):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed) 


sgkf = StratifiedKFold(n_splits=CFG.N_folds, random_state=CFG.seed, shuffle=True)
image_ids = train['image_id'].unique()
stratify_groups = train.groupby('image_id')['category_id'].first()

for fold, (_, test_index) in enumerate(sgkf.split(image_ids, stratify_groups)):
    test_image_ids = image_ids[test_index]
    train.loc[train['image_id'].isin(test_image_ids), "fold"] = fold


def train_epoch(cfg, model, train_loader, optimizer, scheduler, epoch):
    loss_hist = Averager()
    itr = 1
    learning_rate_history = []
    targets = []
    predictions = []
    scaler = amp.GradScaler(enabled=cfg.mixed_precision)
    model.train()
    loss_hist.reset()
    model = model.to(cfg.device, non_blocking=True)

    for images, targets, image_ids in tqdm(train_loader):
        images = [image.to(cfg.device, non_blocking=True) for image in images]
        targets = [{k: v.to(cfg.device, non_blocking=True) for k, v in t.items()} for t in targets]
        
        with amp.autocast(enabled=cfg.mixed_precision):
            loss_dict = model(images, targets)
            
        losses = sum(loss for loss in loss_dict.values())
        loss_value = losses.item()
        loss_hist.send(loss_value)
        
        # Gradient Accumulation
        losses = losses / cfg.gradient_accumulation_steps
        scaler.scale(losses).backward()
        if (itr + 1) % cfg.gradient_accumulation_steps == 0:
            params = [p for p in model.parameters() if p.requires_grad]
            torch.nn.utils.clip_grad_norm_(params, max_norm=cfg.clip_val)
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()
        if itr % 50 == 0:
            print(f"Iteration #{itr} loss: {loss_value}")
        itr += 1
        if scheduler is None:
            lr = optimizer.param_groups[0]['lr']
        else:
            scheduler.step()
            lr = scheduler.get_last_lr()[0]
    print(f"Epoch #{epoch} loss: {loss_hist.value}")
    learning_rate_history.append(lr)
    return learning_rate_history


if __name__ == "__main__":
    
    for FOLD in CFG.train_folds:

        seed_everything(CFG.seed)

        # PREPARE DATA
        fold_train_data = train[train["fold"] != FOLD].reset_index(drop=True)
        fold_valid_data = train[train["fold"] == FOLD].reset_index(drop=True)

        display(
            pd.merge(
                fold_valid_data.groupby(by=["category_id"])["image_id"].count().rename("valid").reset_index(),
                fold_train_data.groupby(by=["category_id"])["image_id"].count().rename("train").reset_index(),
                on="category_id", how="left").T,)


        train_dataset = HouseDataset(fold_train_data, IMG_PATH, get_train_transform())
        valid_dataset = HouseDataset(fold_valid_data, IMG_PATH, get_valid_transform())

        train_dataset.compute_num_categories()
        valid_dataset.compute_num_categories()

        train_loader = DataLoader(
                train_dataset,
                batch_size=CFG.batch_size,
                shuffle=False,
                num_workers=os.cpu_count(),
                pin_memory=True,
                drop_last=True,
                collate_fn=collate_fn
            )

        valid_loader = DataLoader(
            valid_dataset,
            batch_size=CFG.batch_size,
            shuffle=False,
            num_workers=os.cpu_count(),
            pin_memory=True,
            drop_last=False,
            collate_fn=collate_fn
        )
        
        model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights='FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1')
        num_classes = train.category_id.nunique()+1
        in_features = model.roi_heads.box_predictor.cls_score.in_features
        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)

        params = [p for p in model.parameters() if p.requires_grad]
        optimizer = torch.optim.AdamW(params, lr=CFG.lr, weight_decay=CFG.weight_decay)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, eta_min=1e-6, T_max =CFG.epochs * len(train_loader),
            )

        # TRAIN FOLD
        learning_rate_history = []

        print(f"Setting epochs to:{CFG.epochs}....")
        for epoch in range(0, CFG.epochs):
            train_lr = train_epoch(CFG, model, train_loader, optimizer, scheduler, epoch)
            learning_rate_history.extend(train_lr)
        torch.save(model.state_dict(), f'fasternew_docker_model_{FOLD}.pth')
        print(f"......Save model for fold: {FOLD}........ ")
            
            